{
  "nbformat": 4,
  "nbformat_minor": 2,
  "metadata": {
    "kernelspec": {
      "name": "synapse_pyspark",
      "display_name": "Synapse PySpark"
    },
    "language_info": {
      "name": "python"
    },
    "save_output": true,
    "synapse_widget": {
      "version": "0.1",
      "state": {}
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "outputs": [],
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "source": [
        "#Define load and save path\r\n",
        "load_path = f\"abfss://raw@azrawdatalake4djynq.dfs.core.windows.net/Poker Data/PokerGameDetails/\"\r\n",
        "\r\n",
        "save_path = f\"abfss://curated@azcurateddatalake4djynq.dfs.core.windows.net/Poker Data/\"\r\n",
        "\r\n",
        ""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "source": [
        "# Load Dataframe from External Tables created in SQL Serverless"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "outputs": [],
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        },
        "microsoft": {
          "language": "python"
        },
        "collapsed": false
      },
      "source": [
        "%%pyspark\r\n",
        "\r\n",
        "from pyspark.sql.functions import lit\r\n",
        "from pyspark.sql.types import StructType, StructField, StringType, LongType, IntegerType, DecimalType, TimestampType\r\n",
        "\r\n",
        "game_schema = StructType(fields=[StructField(\"GameId\", StringType(), True),\r\n",
        "                                StructField(\"TableName\", StringType(), True),\r\n",
        "                                StructField(\"Board\", StringType(), True),\r\n",
        "                                StructField(\"GameTime\", TimestampType(), True),\r\n",
        "                                StructField(\"GameActionId\", LongType(), True),\r\n",
        "                                StructField(\"Player\", StringType(), True),\r\n",
        "                                StructField(\"StreetName\", StringType(), True),\r\n",
        "                                StructField(\"Action\", StringType(), True),\r\n",
        "                                StructField(\"Amount\", DecimalType(9,2), True),\r\n",
        "                                StructField(\"Cards\", StringType(), True),\r\n",
        "                                StructField(\"StreetSort\", IntegerType(), True)\r\n",
        "])\r\n",
        "\r\n",
        "df = spark.read.load(load_path, format='parquet', schema=game_schema)\r\n",
        "\r\n",
        "df = df.withColumn(\"year\",df.GameTime.substr(1,4)) \\\r\n",
        "    .withColumn(\"month\", df.GameTime.substr(6, 2)) \\\r\n",
        "    .withColumn(\"day\", df.GameTime.substr(9,2)) \r\n",
        "    \r\n",
        "display(df.limit(10))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "source": [
        "# Persist transformed data back to lake"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "outputs": [],
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        },
        "microsoft": {
          "language": "python"
        }
      },
      "source": [
        "%%pyspark\r\n",
        "\r\n",
        "#Write data to target location partition by year, month, & day in Delta\r\n",
        "from pyspark.sql.functions import lit\r\n",
        "\r\n",
        "gd_file_path = save_path + \"PokerGameDetails/\"\r\n",
        "\r\n",
        "df_output = spark.read.load(load_path, format='parquet', schema=game_schema)\r\n",
        "\r\n",
        "df_output = df_output.withColumn(\"year\",df_output.GameTime.substr(1,4)) \\\r\n",
        "    .withColumn(\"month\", df_output.GameTime.substr(6, 2)) \\\r\n",
        "    .withColumn(\"day\", df_output.GameTime.substr(9,2)) \r\n",
        "\r\n",
        "df_output.write.partitionBy(\"year\",\"month\",\"day\") \\\r\n",
        "    .format(\"delta\") \\\r\n",
        "    .mode(\"overwrite\") \\\r\n",
        "    .save(gd_file_path)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "source": [
        "# Create SQL View over new dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "outputs": [],
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "source": [
        "df_output.createOrReplaceTempView('vwDeltaGameDetails')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "source": [
        "Create a temporary view in the Spark database to simplify shaping the data.  Here we are looking at individual game details and flagging whether the player\n",
        "voluntarily put money in the pot, or did a preflop raise."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "outputs": [],
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        },
        "microsoft": {
          "language": "sparksql"
        },
        "collapsed": false
      },
      "source": [
        "%%sql\n",
        "CREATE OR REPLACE TEMPORARY VIEW PlayerGameSummary AS \n",
        "WITH T1 AS (\n",
        "SELECT player, GameId, gameTime\n",
        "\t, CAST(gameTime AS DATE) AS GameDate\n",
        "\t, MAX(\n",
        "\t\t\tCASE WHEN action = 'bet' THEN 1\n",
        "\t\t\t\tWHEN action = 'raise' THEN 1\n",
        "\t\t\t\tWHEN action = 'call' THEN 1\n",
        "\t\t\t\tELSE 0\n",
        "\t\t\t\tEND \n",
        "\t\t ) AS vpipct\n",
        "\t, MAX(\n",
        "\t\t\tCASE WHEN action = 'raise' THEN 1 \n",
        "\t\t\t\tELSE 0\n",
        "\t\t\t\tEND\n",
        "\t\t) AS pfrCt\n",
        "\t, 1 AS GameCt\n",
        "FROM vwDeltaGameDetails\n",
        "WHERE streetName = 'Pre-flop'\n",
        "GROUP BY player, GameId, gameTime\n",
        ")\n",
        ", T2 AS (\n",
        "SELECT player, GameId, gameTime\n",
        "\t, SUM(amount) AS Profit\n",
        "FROM vwDeltaGameDetails\n",
        "GROUP BY player, GameId, gameTime\n",
        ")\n",
        "SELECT CAST(T1.GameId AS VARCHAR(100)) AS GameId\n",
        "\t, CAST(T1.Player AS VARCHAR(100)) AS Player\n",
        "\t, T1.GameDate\n",
        "\t, SUM(T1.vpipct) AS vpipCt\n",
        "\t, SUM(T1.pfrct)  AS pfrCt\n",
        "\t, SUM(T1.GameCt) AS GameCt\n",
        "\t, SUM(T2.Profit) AS Winnings\n",
        "FROM T1\n",
        "INNER JOIN T2 ON T1.GameId = T2.GameId AND\n",
        "\tT1.Player = T2.Player\n",
        "GROUP BY T1.GameId, T1.Player, T1.GameDate"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "outputs": [],
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import plotly\n",
        "import random\n",
        "import plotly .express as px\n",
        "from sklearn import preprocessing\n",
        "from sklearn.cluster import KMeans\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "\n",
        "df = spark.sql(\"SELECT  player, SUM(vpipCt) as VPIP_Ct, SUM(pfrCt) as PFR_Ct, SUM(gameCt) as Game_Ct, SUM(Winnings) as Winnings, (SUM(Winnings)/SUM(gameCt))*100 as WinningsPer100 FROM PlayerGameSummary where player <> 'Player000009' GROUP BY player\").toPandas()\n",
        "df.sort_values(by = 'player', inplace = True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "outputs": [],
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "source": [
        "# Manufacture new columns in the dataset\n",
        "\n",
        "#PFR -> How often does the player raise before the flop\n",
        "df['PFR'] = df['PFR_Ct']/df['Game_Ct']\n",
        "#VP$P -> How often does the player voluntarily put money in the pot (ie. bet, call, raise, but not blind)\n",
        "df['VP$IP'] = df['VPIP_Ct']/df['Game_Ct']\n",
        "#pfr_rate -> When a player is betting on a game how often is it a pre flop raise.  This is a measure of aggression.\n",
        "df['pfr_rate'] = df['PFR'] / df['VP$IP']\n",
        "df['playertype'] = ''\n",
        "df['labels'] = -1\n",
        "# Fill in blanks\n",
        "df['pfr_rate'] = df['pfr_rate'].fillna(0)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "outputs": [],
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "source": [
        "#split out the 'Silver' player types from the 'Bronze'\n",
        "df_S = df[(df['Game_Ct'] >= 500) & (df['PFR_Ct'] > 0) & (df['VPIP_Ct'] > 0)].copy()\n",
        "df_S['playertype'] = 'Silver'\n",
        "df_B = df[(df['Game_Ct'] < 500) | (df['PFR_Ct'] == 0) | (df['VPIP_Ct'] == 0)].copy()\n",
        "df_B['playertype'] = 'Bronze'"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "outputs": [],
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "source": [
        "np.random.seed(42)\n",
        "#Limit attributes for K-Means clustering\n",
        "attributes = ['pfr_rate','VP$IP']\n",
        "\n",
        "# Create DataFrame for K Means Model\n",
        "x = df_S[attributes]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "outputs": [],
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "source": [
        "#Measure distortion to see the optimal number of logical clusters in the data\n",
        "\n",
        "\n",
        "distortions = []\n",
        "for i in range(1, 11):\n",
        "    km = KMeans(\n",
        "        n_clusters=i, init='k-means++',\n",
        "        n_init=25, max_iter=300,\n",
        "        tol=1e-04, random_state=42\n",
        "    )\n",
        "    km.fit(x)\n",
        "    distortions.append(km.inertia_)\n",
        "\n",
        "# plot\n",
        "plt.plot(range(1, 11), distortions, marker='o')\n",
        "plt.xlabel('Number of clusters')\n",
        "plt.ylabel('Distortion')\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "outputs": [],
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "source": [
        "#SPectral Clustering gave better results \n",
        "import pickle\n",
        "from sklearn.cluster import SpectralClustering\n",
        "\n",
        "# set seed\n",
        "np.random.seed(42)\n",
        "# define the model\n",
        "model = SpectralClustering(n_clusters=4, random_state = 42)\n",
        "# fit the model\n",
        "model.fit(x)\n",
        "\n",
        "#save model\n",
        "model_s = pickle.dumps(model)\n",
        "\n",
        "#load model\n",
        "model_l = pickle.loads(model_s)\n",
        "\n",
        "# assign a cluster to each example\n",
        "yhat = model_l.fit_predict(x)\n",
        "\n",
        "\n",
        "df_S['labels'] = yhat\n",
        "df_S['labels'] = df_S['labels'].astype('category')\n",
        "df_S['Winnings'] = df_S['Winnings'].astype('float64')\n",
        "fig = px.scatter(df_S, x = \"pfr_rate\", y = \"VP$IP\", color = 'labels', title = str('Cluster Algorithm: ' + 'Spectral Clustering'))\n",
        "alg = plotly.offline.plot(fig, output_type='div')\n",
        "displayHTML(alg)\n",
        "\n",
        ""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "outputs": [],
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "source": [
        "print('Calling Station =  low pfr_rate and high VP$IP')\n",
        "print('Tight Aggressive = high pfr_rate and low VP$IP')\n",
        "print('Rock = low pfr_rate and low VP$IP')\n",
        "print('Loose Aggressive = high pfr_rate and high VP$IP')\n",
        "print()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "outputs": [],
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        },
        "collapsed": false
      },
      "source": [
        "df_S_agg = df_S.groupby('labels', as_index= False)[['Winnings','Game_Ct','pfr_rate','VP$IP']].agg(['count','sum','mean'])\n",
        "df_S_agg.columns = df_S_agg.columns.map(lambda x: f'{x[0]}_{x[1]}')\n",
        "df_S_agg['Avg_winnings'] = (df_S_agg['Winnings_sum']/df_S_agg['Game_Ct_sum'])*100\n",
        "agg_col_select = ['Winnings_count', 'pfr_rate_mean', 'VP$IP_mean', 'Avg_winnings','Winnings_sum'  ]\n",
        "df_S_summary = df_S_agg[agg_col_select].reset_index()\n",
        "display(df_S_summary)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "outputs": [],
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "source": [
        "#Get the two attributes from the noobs dataframe and predict\n",
        "y = df_B[attributes] \n",
        "#labels = model.fit_predict(y)\n",
        "labels = 99   # for all the players with less than 500 games or 'PFR_Ct' == 0 or 'VPIP_Ct' == 0\n",
        "\n",
        "#add the labels into the noobs dataframe\n",
        "df_B['labels'] = labels\n",
        "df_B['labels'] = df_B['labels'].astype('category')\n",
        "df_B['Winnings'] = df_B['Winnings'].astype('float64')\n",
        "print(df_B.head())"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "source": [
        "# Persist the dataframe\n",
        "\n",
        "Convert the pandas dataframe to Spark, then save it as table to Spark, which will create the underlying delta structure."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "outputs": [],
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "source": [
        "#Merge the data frames before persisting the results\n",
        "df_allplayers = df_S.append(df_B)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "outputs": [],
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "source": [
        "#Convert the dataframe from pandas -> spark.\n",
        "df_output = spark.createDataFrame(df_allplayers)\n",
        "\n",
        "#Output the data frame as a spark table.   We expect to run this often so overwrite whatever is there.\n",
        "\n",
        "ps_file_path = save_path + \"PlayerSummary/\"\n",
        "df_output.write.format('delta').mode('overwrite').save(ps_file_path)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "source": [
        "# Persist model data to the Lake database table\n",
        "\n",
        "Create the Lake database [pokerdata] and persists the model to the table [pokerdata].[playersummary]."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "outputs": [],
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        },
        "microsoft": {
          "language": "sparksql"
        },
        "collapsed": false
      },
      "source": [
        "%%sql\n",
        "CREATE DATABASE IF NOT EXISTS lakePokerData"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "outputs": [],
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        },
        "microsoft": {
          "language": "python"
        }
      },
      "source": [
        "%%pyspark\n",
        "df = spark.read.load(ps_file_path, format='delta')\n",
        "df.write.mode(\"overwrite\").saveAsTable(\"lakePokerData.playerSummary\")"
      ]
    }
  ]
}